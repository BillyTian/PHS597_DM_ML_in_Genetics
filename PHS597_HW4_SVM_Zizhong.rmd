---
title: "PHS597 HW#4 SVM"
author: "Zizhong Tian"
date: "11/12/2022"
output: html_document
---

## Support vector machine (SVM) WITHOUT the slack variable

The objective function is 
$$\frac{1}{2}||\beta||^2,~~\text{subject to}~ y_i(\beta^Tx_i+\beta_0)\ge 1~\text{for}~i=1,..., N$$
Applying KKT condition, we have the multiplier
$$L(\beta, \beta_0, \alpha)=\frac{1}{2}||\beta||^2+\sum_i\alpha_ih_i(\beta, \beta_0),~~\text{where}~h_i(\beta,\beta_0)=1-y_i(\beta^Tx_i+\beta_0)\le 0,~\alpha_i\ge 0$$
and the equivalent objective based on the multiplier is
$$\text{min}_{\beta, \beta_0}\text{max}_{\alpha\ge 0}L(\beta, \beta_0, \alpha)$$
After setting the derivatives with respect to $\beta$ and $\beta_0$ to 0, we get
$$\beta=\sum_i\alpha_iy_ix_i,~~\sum_i\alpha_iy_i=0,$$
and $\alpha_i=0$ when $y_i(\beta^Tx_i+\beta_0)>1$.
Plugging in the two conditions above allow us to get
$$L(\alpha)=-\frac{1}{2}\sum_i\sum_{i'}\alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}+\sum_i\alpha_i$$
The optimization problem becomes
$$min_{\alpha}\{-L(\alpha)\}=min_{\alpha}\left\{\frac{1}{2}\sum_i\sum_{i'}\alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}-\sum_i\alpha_i\right\},~~\text{subject to}~\alpha\ge0,~\sum_i\alpha_iy_i=0$$
To further simplify the form, we can get
$$min_{\alpha}\left\{-1^T\alpha+\frac{1}{2}\alpha^TA^TA\alpha\right\},~~\text{where}~A=\begin{bmatrix}y_1x_{11} & y_2x_{12} & ... & y_nx_{1n}\\
y_1x_{21} & y_2x_{22}& ... & y_nx_{2n}\\
... & ... & ... & ...\\
y_1x_{p1} & y_2x_{p2} & ... & y_nx_{pn}\end{bmatrix}$$
with the constraint
$$y^T\alpha=0,~~\begin{bmatrix}
1 &0&...&0\\
0&1&...&0\\
...&...&...&...\\
0&0&...&1\end{bmatrix}\alpha\ge\begin{bmatrix}
0\\
0\\
...\\
0\end{bmatrix}$$

Simulate a separable-case data:
```{r, warning=F, message=F}
#Simulate a dataset
n <- 20
set.seed(597)
X1 <- rnorm(n, mean=1, sd=1)
X2 <- rnorm(n, mean=2, sd=2)
#epsilon <- rnorm(n, mean=0, sd=1)

beta0 <- 1
beta1 <- 0.8
beta2 <- -0.6

#set a true boundary line
L <- beta0 + beta1*X1 + beta2*X2

Y <- ifelse(L>0, 1, -1)
#table(Y)
```
Implement the SVM algorithm without slack variable:
```{r, message=F, warning=F}
library(quadprog)
library(Matrix)

SVM_noslack <- function(X1, X2, Y){
  bvec <- rep(0, 1+n)
  Amat <- as.matrix(rbind(Y, diag(1, nrow=n)))

  dvec <- rep(1, n)
  A <- as.matrix(rbind(Y*X1, Y*X2))
  Dmat <- t(A) %*% A
  
  #X <- cbind(X1, X2)
  #Dmat <- matrix(NA, nrow=n, ncol=n)
  #for (i in 1:n){
  #  for (j in 1:n){
  #    Dmat[i,j] <- Y[i]*Y[j]*as.numeric(t(as.numeric(X[i,])) %*% as.numeric(X[j,]))
  #  }
  #}
  
  pd_Dmat <- nearPD(Dmat)
  Dmat <- pd_Dmat$mat
  
  #sc <- norm(Dmat,"2")
  
  test <- solve.QP(Dmat=Dmat, 
                   dvec=dvec, 
                   Amat=t(Amat), 
                   bvec=bvec,
                   meq=1)
  alpha_sol <- test$solution
  beta_sol <- c(sum(alpha_sol*Y*X1), sum(alpha_sol*Y*X2))
  #beta_sol
  
  alpha_ind <- which(alpha_sol>1e-6)[1] #??
  beta0_sol <- as.numeric( Y[alpha_ind]-t(beta_sol) %*% c(X1[alpha_ind],X2[alpha_ind]) )
  #beta0_sol
  return(c(beta0_sol, beta_sol))
}

beta_noslack <- SVM_noslack(X1, X2, Y)
beta0_sol <- beta_noslack[1]
beta_sol <- beta_noslack[2:3]
```
As shown below, the prediction accuracy is $100\%$. The underlying true boundary (green) and the predicted boundary based on the algorithm (blue) are shown on the plot.
```{r}
pred_Y <- ifelse(beta0_sol+as.numeric(cbind(X1, X2)%*%beta_sol)>0, 1, -1)
#mean(Y==pred_Y)

Y_cat <- as.factor(Y)
est_X2 <- 1/beta2 * (-beta0-beta1*X1)

pred1_X2 <- 1/beta_sol[2] * (-beta0_sol-beta_sol[1]*X1)

plot(X1, X2, type="p", col=Y_cat)
lines(X1, est_X2, type="l", lty=2, col="green3")
lines(X1, pred1_X2, type="l", lty=1, col="blue")
```


## SVM WITH the slack variable

The objective function is 
$$C\sum_i\xi_i+\frac{1}{2}||\beta||^2,~~\text{subject to}~ y_i(\beta^Tx_i+\beta_0)\ge 1-\xi_i,~\xi_i\ge 0$$
Applying KKT condition, we have
$$L(\beta, \beta_0, \alpha,\mu)=\frac{1}{2}||\beta||^2+C\sum_i\xi_i-\sum_i\alpha_i[y_i(\beta^Tx_i+\beta_0)-1+\xi_i]-\sum_i\mu_i\xi_i$$
and the equivalent objective based on the multiplier is
$$\text{min}_{\beta, \beta_0}\text{max}_{\mu}\text{max}_{\alpha\ge 0}L(\beta, \beta_0, \alpha,\mu)$$
After setting the derivatives with respect to $\beta$, $\beta_0$, and $\xi_i$ to 0, we have
$$\beta=\sum_i\alpha_iy_ix_i,~~\sum_i\alpha_iy_i=0,~~\alpha_i=C-\mu_i$$
with $\alpha_i\ge 0$ and $\mu_i\ge 0$ ($\alpha_i=0$ when $y_i(\beta^Tx_i+\beta_0)>1$).
Plugging in the conditions above allow us to get
$$L(\alpha)=-\frac{1}{2}\sum_i\sum_{i'}\alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}+\sum_i\alpha_i$$
The optimization problem becomes
$$min_{\alpha}\{-L(\alpha)\}=min_{\alpha}\left\{-\sum_i\alpha_i+\frac{1}{2}\sum_i\sum_{i'}\alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}\right\},~~\text{subject to}~\alpha\in[0,C],~\sum_i\alpha_iy_i=0$$
To further simplify the form, we can get
$$min_{\alpha}\left\{-1^T\alpha+\frac{1}{2}\alpha^TA^TA\alpha\right\},~~\text{where}~A=\begin{bmatrix}y_1x_{11} & y_2x_{12} & ... & y_nx_{1n}\\
y_1x_{21} & y_2x_{22}& ... & y_nx_{2n}\\
... & ... & ... & ...\\
y_1x_{p1} & y_2x_{p2} & ... & y_nx_{pn}\end{bmatrix}$$
with the constraint
$$y^T\alpha=0,~~\begin{bmatrix}
1 &0&...&0\\
0&1&...&0\\
...&...&...&...\\
0&0&...&1\\
-1 &0&...&0\\
0&-1&...&0\\
...&...&...&...\\
0&0&...&-1\end{bmatrix}\alpha\ge\begin{bmatrix}
0\\
0\\
...\\
0\\
-C\\
-C\\
...\\
-C\end{bmatrix}$$

Simulate a data of the non-separable case:
```{r}
#Simulate a dataset
n <- 20
set.seed(597)
X1 <- rnorm(n, mean=1, sd=1)
X2 <- rnorm(n, mean=2, sd=2)
epsilon <- rnorm(n, mean=0, sd=1)

beta0 <- 1
beta1 <- 0.8
beta2 <- -0.6

L2 <- beta0 + beta1*X1 + beta2*X2 + epsilon

Y2 <- ifelse(L2>0, 1, -1)
#table(Y2)

Y2_cat <- as.factor(Y2)
est_X2 <- 1/beta2 * (-beta0-beta1*X1)

plot(X1, X2, type="p", col=Y2_cat)
lines(X1, est_X2, type="l", lty=2, col="green3")
```


Implement the SVM algorithm with slack variable:
```{r}
SVM_slack <- function(X1, X2, Y, C){
  bvec2 <- c(rep(0, 1+n), rep(-C, n))
  Amat2 <- as.matrix(rbind(Y2, diag(1, nrow=n), diag(-1, nrow=n)))
  
  dvec2 <- rep(1, n)
  A2 <- as.matrix(rbind(Y2*X1, Y2*X2))
  Dmat2 <- t(A2) %*% A2
  pd_Dmat2 <- nearPD(Dmat2)
  Dmat2 <- as.matrix(pd_Dmat2$mat)
  #sc <- norm(Dmat,"2")
  
  test2 <- solve.QP(Dmat=Dmat2, 
                    dvec=dvec2, 
                    Amat=t(Amat2), 
                    bvec=bvec2,
                    meq=1)
  alpha_sol2 <- test2$solution
  beta_sol2 <- c(sum(alpha_sol2*Y2*X1), sum(alpha_sol2*Y2*X2))
  #beta_sol2
  
  alpha_ind2 <- which(alpha_sol2>=1e-6 & alpha_sol2<(C-0.01))[1]
  
  beta0_sol2 <- as.numeric( Y2[alpha_ind2]-t(beta_sol2) %*% c(X1[alpha_ind2],X2[alpha_ind2]) )
  #beta0_sol2
  return(c(beta0_sol2, beta_sol2))
}


beta_slack <- SVM_slack(X1=X1, X2=X2, Y=Y2, C=2)
beta0_sol2 <- beta_slack[1]
beta_sol2 <- beta_slack[2:3]
```
The classification performance is shown below:
```{r}
pred_Y2 <- ifelse(beta0_sol2+as.numeric(cbind(X1, X2)%*%beta_sol2)>0, 1, -1)
#mean(Y2==pred_Y2)

pred2_X2 <- 1/beta_sol2[2] * (-beta0_sol2-beta_sol2[1]*X1)

plot(X1, X2, type="p", col=Y2_cat)
#lines(X1, est_X2, type="l", lty=2, col="green3")
lines(X1, pred2_X2, type="l", lty=1, col="blue")
```


### An example that employing the slack variable will INCREASE the prediction accuracy
If using the data from the slack SVM example, it shows that the algorithm with slack variable ($80\%$) has higher performance compared to that without the slack variable ($65\%$).
```{r}
beta_noslack_2 <- SVM_noslack(X1=X1, X2=X2, Y=Y2)
beta0_sol1_2 <- beta_noslack_2[1]
beta_sol1_2 <- beta_noslack_2[2:3]

pred_Y_2 <- ifelse(beta0_sol1_2+as.numeric(cbind(X1, X2)%*%beta_sol1_2)>0, 1, -1)
#mean(Y==pred_Y_2)

pred1_X2_2 <- 1/beta_sol1_2[2] * (-beta0_sol1_2-beta_sol1_2[1]*X1)

plot(X1, X2, type="p", col=Y2_cat)
lines(X1, pred1_X2_2, type="l", lty=1, col="red")
lines(X1, pred2_X2, type="l", lty=1, col="blue")
legend("topleft",c("without slack", "with slack"), col=c("red", "blue"), lty=c(1,1))
```


### An example that employing the slack variable will DECREASE the prediction accuracy
If using the data from the non-slack SVM example, it shows that the algorithm with slack variable decrease the accuracy from $100\%$ to $80\%$.
```{r}
beta_slack_2 <- SVM_slack(X1=X1, X2=X2, Y=Y, C=2)
beta0_sol2_2 <- beta_slack_2[1]
beta_sol2_2 <- beta_slack_2[2:3]

pred_Y2_2 <- ifelse(beta0_sol2_2+as.numeric(cbind(X1, X2)%*%beta_sol2_2)>0, 1, -1)
mean(Y==pred_Y2_2)

pred2_X2_2 <- 1/beta_sol2_2[2] * (-beta0_sol2_2-beta_sol2_2[1]*X1)

plot(X1, X2, type="p", col=Y_cat)
lines(X1, pred1_X2, type="l", lty=1, col="red")
lines(X1, pred2_X2_2, type="l", lty=1, col="blue")
legend("topleft",c("without slack", "with slack"), col=c("red", "blue"), lty=c(1,1))
```


## SVM Regression

The objective function after introducing slack variables is 
$$\frac{1}{2}||\beta||^2+C\sum_{i=1}(\xi_i+\xi_i^*)$$
subject to 
$$y_i-(\beta^Tx_i+\beta_0)\le\epsilon+\xi_i,~~-y_i+(\beta^Tx_i+\beta_0)\le\epsilon+\xi_i^*,~~\xi_i,\xi_i^*\ge0$$
Applying KKT condition, we have
$$L=\frac{1}{2}||\beta||^2+C\sum_i(\xi_i+\xi_i^*)-\sum_i(\eta_i\xi_i+\eta_i^*\xi_i^*)$$
$$-\sum_i\alpha_i[\epsilon+\xi_i-y_i+(\beta^Tx_i+\beta_0)]-\sum_i\alpha_i^*[\epsilon+\xi_i^*+y_i-(\beta^Tx_i+\beta_0)]$$
After setting the derivatives with respect to the parameters to zeros, we have
$$\sum_i(\alpha_i-\alpha_i^*)=0,~~\beta=\sum_i(\alpha_i-\alpha_i^*)x_i,~~\alpha_i=C-\eta_i~~\alpha_i^*=C-\eta_i^*$$
After some organizing, the dual formula is
$$L(\alpha)=-\frac{1}{2}\sum_{i=1}\sum_{j=1}(\alpha_i-\alpha_i^*)(\alpha_j-\alpha_j^*)x_{i}^Tx_j-\epsilon\sum_{i=1}(\alpha_i+\alpha_i^*)+\sum_{i=1}y_i(\alpha_i-\alpha_i^*)$$
subject to 
$$\sum_i(\alpha_i-\alpha_i^*)=0,~\alpha_i\in[0,C],~\alpha_i^*\in [0,C]$$
Define $\alpha=[\alpha_1,...,\alpha_n,\alpha_1^*,...\alpha_n^*]^T$. Then, 
$$B\alpha=\begin{bmatrix}\alpha_1-\alpha_1^*\\...\\\alpha_n-\alpha_n^*\end{bmatrix},~~\text{where}~B=\begin{bmatrix}1&0&...&0&-1&0&...&0\\...&...&...&...&...&...&...&...\\0&...&0 & 1&0&...&0&-1\end{bmatrix}=[I_{n\times n}, -I_{n\times n}]$$
Define $X$ to be the $p\times n$ covariate matrix. The optimization problem can be simplified into
$$\text{min}_\alpha\left\{-L(\alpha)\right\}=\text{min}_\alpha\left\{\frac{1}{2}\alpha^TB^TX^TXB\alpha+\epsilon1^T\alpha-Y_1\alpha+Y_2\alpha\right\},~~\text{where}~Y_1=[y_1,...,y_n,0,...,0],~Y_2=[0,...,0,y_1,...,y_n]$$
with constraints
$$1^TB\alpha=0,~\begin{bmatrix}
I_{2n\times 2n}\\
-I_{2n\times 2n}\end{bmatrix}\alpha\ge\begin{bmatrix}
0\\
-C\end{bmatrix}$$
Finally, I calculate
$$\beta_0=\frac{1}{2}\left(\text{max}\{-\epsilon+y_i-\beta^Tx_i~|~\alpha_i<C~\text{or}~\alpha_i^*>0\}+\text{min}\{-\epsilon+y_i-\beta^Tx_i~|~\alpha_i>0~\text{or}~\alpha_i^*<C\}\right)$$
(Some derivation steps for the dual form were referenced from https://alex.smola.org/papers/2004/SmoSch04.pdf.)$\\$

Simulate a dataset with one covariate:
```{r}
#Simulate a dataset
n <- 20
set.seed(597)
X1 <- rnorm(n, mean=1, sd=1)
epsilon <- rnorm(n, mean=0, sd=0.3)

beta0 <- 1
beta1 <- 0.8

Y3 <- beta0 + beta1*X1 + epsilon

est_Y3 <- beta0 + beta1*X1

plot(X1, Y3, type="p")
lines(X1, est_Y3, type="l", lty=2, col="green3")

```


Implement SVM regression:
```{r}
C <- 2
eps <- 0.001

bvec3 <- c(rep(0, 1+2*n), rep(-C, 2*n))

B <- cbind(diag(1, nrow=n), diag(-1, nrow=n))

Amat3 <- as.matrix(rbind(as.numeric(t(rep(1,n))%*%B), diag(1, nrow=2*n), diag(-1, nrow=2*n)))

dvec3 <- -eps*rep(1, 2*n)+c(Y3,rep(0,n))-c(rep(0,n),Y3)

Dmat3 <- t(B) %*% X1 %*% t(X1) %*% B
pd_Dmat3 <- nearPD(Dmat3)
Dmat3 <- as.matrix(pd_Dmat3$mat)

test3 <- solve.QP(Dmat=Dmat3, 
                  dvec=dvec3, 
                  Amat=t(Amat3), 
                  bvec=bvec3,
                  meq=1)
alpha_sol3 <- test3$solution
alpha1 <- alpha_sol3[1:n]
alpha2 <- alpha_sol3[(n+1):(2*n)]
beta_sol3 <- as.numeric(t(alpha1-alpha2) %*% X1)
#beta_sol3

alpha_ind1 <- which(alpha2>1e-6 | alpha1<1.9999)
lower <- max(-eps + Y3[alpha_ind1]-beta_sol3*X1[alpha_ind1])

alpha_ind2 <- which(alpha1>1e-6 | alpha2<1.9999)
upper <- min(-eps + Y3[alpha_ind2]-beta_sol3*X1[alpha_ind2])

#alpha_ind3 <- which(alpha1>=1e-6 & alpha1<1.9999 & alpha2>=1e-6 & alpha2<1.9999)
#beta0_sol3 <- as.numeric( Y3[alpha_ind3]-beta_sol3*X1[alpha_ind3] )

beta0_sol3 <- (lower+upper)/2
```
The predicted trajectory is illustrated in the plot below:
```{r}
pred_Y3 <- beta0_sol3 + beta_sol3*X1

plot(X1, Y3, type="p")
lines(X1, est_Y3, type="l", lty=2, col="green3")
lines(X1, pred_Y3, type="l", lty=1, col="blue")
```
